{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bACfZHSJ3o8A",
        "outputId": "efbb945e-2e61-4c24-c417-6dac35b165dd"
      },
      "outputs": [],
      "source": [
        "#!pip install pathlib2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rtHVti8i9NV4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Tuple, Any\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score as accuracy, f1_score\n",
        "from sklearn.preprocessing import scale\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from pathlib2 import Path\n",
        "from os.path import join\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-29abgOMBM3"
      },
      "source": [
        "**Yahoo Finance Setup**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "715nH7ZNQ05Q"
      },
      "source": [
        "Library Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cnqAlas7Ht92"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLYm9by6RFHI"
      },
      "source": [
        "**TA LIB setup**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b7BraYdScAD",
        "outputId": "5154862d-afbd-4868-f666-9717a493f420"
      },
      "outputs": [],
      "source": [
        "#url = 'https://anaconda.org/conda-forge/libta-lib/0.4.0/download/linux-64/libta-lib-0.4.0-h166bdaf_1.tar.bz2'\n",
        "#!curl -L $url | tar xj -C /usr/lib/x86_64-linux-gnu/ lib --strip-components=1\n",
        "#url = 'https://anaconda.org/conda-forge/ta-lib/0.4.19/download/linux-64/ta-lib-0.4.19-py310hde88566_4.tar.bz2'\n",
        "#!curl -L $url | tar xj -C /usr/local/lib/python3.10/dist-packages/ lib/python3.10/site-packages/talib --strip-components=3\n",
        "import talib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwSKAYBxF6WH"
      },
      "source": [
        "**Fred API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaDHplvbGAJT",
        "outputId": "59ef5c95-4421-4299-8cc2-d3a2d8f2cf94"
      },
      "outputs": [],
      "source": [
        "#!pip install fredapi\n",
        "from fredapi import Fred\n",
        "fred = Fred(api_key='X')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x0Re73dPgsr"
      },
      "source": [
        "**Investing.com API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_EADrcvPgWa",
        "outputId": "1ce040c3-bb1a-41ba-eb63-a17465494795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting investiny\n",
            "  Downloading investiny-0.7.2-py3-none-any.whl (9.2 kB)\n",
            "Collecting httpx<0.24.0,>=0.23.0 (from investiny)\n",
            "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<2.0.0,>=1.10.2 (from investiny)\n",
            "  Downloading pydantic-1.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.24.0,>=0.23.0->investiny) (2024.2.2)\n",
            "Collecting httpcore<0.17.0,>=0.15.0 (from httpx<0.24.0,>=0.23.0->investiny)\n",
            "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986[idna2008]<2,>=1.3 (from httpx<0.24.0,>=0.23.0->investiny)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.24.0,>=0.23.0->investiny) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0,>=1.10.2->investiny) (4.11.0)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore<0.17.0,>=0.15.0->httpx<0.24.0,>=0.23.0->investiny)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.17.0,>=0.15.0->httpx<0.24.0,>=0.23.0->investiny) (3.7.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from rfc3986[idna2008]<2,>=1.3->httpx<0.24.0,>=0.23.0->investiny) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.17.0,>=0.15.0->httpx<0.24.0,>=0.23.0->investiny) (1.2.0)\n",
            "Installing collected packages: rfc3986, pydantic, h11, httpcore, httpx, investiny\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.7.0\n",
            "    Uninstalling pydantic-2.7.0:\n",
            "      Successfully uninstalled pydantic-2.7.0\n",
            "Successfully installed h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 investiny-0.7.2 pydantic-1.10.15 rfc3986-1.5.0\n"
          ]
        }
      ],
      "source": [
        "#!pip install investiny\n",
        "#from investiny import historical_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLewjRlyBWAn"
      },
      "source": [
        "Helper **Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9e5ahdapBVoq"
      },
      "outputs": [],
      "source": [
        "def go_back_n_days(date_str, n):\n",
        "    \"\"\"\n",
        "    This function takes a date string in the format \"YYYY-MM-DD\" and an integer n,\n",
        "    and returns a new date string representing the date n days before the given date.\n",
        "\n",
        "    Args:\n",
        "    date_str (str): The input date string in the format \"YYYY-MM-DD\".\n",
        "    n (int): The number of days to go back from the given date.\n",
        "\n",
        "    Returns:\n",
        "    str: A date string in the format \"YYYY-MM-DD\" of the new date.\n",
        "    \"\"\"\n",
        "    # Parse the input date string\n",
        "    date_format = \"%Y-%m-%d\"\n",
        "    current_date = datetime.strptime(date_str, date_format).date()\n",
        "\n",
        "    # Compute the date n days before\n",
        "    new_date = current_date - timedelta(days=n)\n",
        "\n",
        "    # Return the new date in the same string format\n",
        "    return new_date.strftime(date_format)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqCusj2Z8ZgK"
      },
      "source": [
        "**Primative Data Download**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hfnXp14I8SpT"
      },
      "outputs": [],
      "source": [
        "INDEX = [\"^DJI\", \"^IXIC\", \"^NYA\", \"^RUT\", \"^GSPC\"]\n",
        "start_date = \"2009-12-31\"\n",
        "end_date = \"2019-12-01\"\n",
        "\n",
        "data = {}\n",
        "\n",
        "for index in INDEX:\n",
        "    data[index] = yf.download(index, start=start_date, end=end_date, progress=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD8oaPQZRA6e"
      },
      "source": [
        "*V2 - Closing price* - Included"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRDhDuru71FG"
      },
      "source": [
        "*V3 - Relative change of volume*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nYfTd4WsXtHq"
      },
      "outputs": [],
      "source": [
        "\n",
        "vol_relative_change = {}\n",
        "for index, df in data.items():\n",
        "    # Ensure volume is float for calculations\n",
        "    df['Volume'] = df['Volume'].astype(float)\n",
        "    # Calculate the relative change of volume (current volume divided by previous day's volume)\n",
        "    df[\"Vol\"] = (df['Volume'] / df['Volume'].shift(1)) - 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTl9CnIqEOxA"
      },
      "source": [
        "*V4 to V14*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nrSBf5w4Bxc-"
      },
      "outputs": [],
      "source": [
        "for index, df in data.items():\n",
        "    # Calculate Momentum (MOM) for 1, 2, and 3 days before\n",
        "    df['mom'] = df['Close'].pct_change(periods=1)  #NOT INCLUDED IN TABLE!!!**********\n",
        "    df['mom1'] = df['mom'].shift(1)  # Shifted 1 day to get the return of 2 days ago\n",
        "    df['mom2'] = df['mom'].shift(2)  # Shifted 2 days to get the return of 3 days ago\n",
        "    df['mom3'] = df['mom'].shift(3)  # Shifted 3 days to get the return of 4 days ago\n",
        "\n",
        "\n",
        "    # Calculate Rate of Change (ROC) for 5, 10, 15, and 20 days\n",
        "    df['ROC_5'] = talib.ROC(df['Close'], timeperiod=5)\n",
        "    df['ROC_10'] = talib.ROC(df['Close'], timeperiod=10)\n",
        "    df['ROC_15'] = talib.ROC(df['Close'], timeperiod=15)\n",
        "    df['ROC_20'] = talib.ROC(df['Close'], timeperiod=20)\n",
        "\n",
        "    # Calculate Exponential Moving Average (EMA) for 10, 20, 50, and 200 days\n",
        "    df['EMA_10'] = talib.EMA(df['Close'], timeperiod=10)\n",
        "    df['EMA_20'] = talib.EMA(df['Close'], timeperiod=20)\n",
        "    df['EMA_50'] = talib.EMA(df['Close'], timeperiod=50)\n",
        "    df['EMA_200'] = talib.EMA(df['Close'], timeperiod=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aVQLTDpLF3G"
      },
      "source": [
        "*V15 to V31*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MgJMixTrQC5C"
      },
      "outputs": [],
      "source": [
        "for index, df in data.items():\n",
        "  #V15\n",
        "  df[\"DTB4WK\"] = fred.get_series(\"DTB4WK\", observation_start=start_date, observation_end=end_date)\n",
        "  #V16\n",
        "  df[\"DTB3\"] = fred.get_series(\"DTB3\", observation_start=start_date, observation_end=end_date)\n",
        "  #V17\n",
        "  df[\"DTB6\"] = fred.get_series(\"DTB6\", observation_start=start_date, observation_end=end_date)\n",
        "  #V18\n",
        "  df[\"DGS5\"] = fred.get_series(\"DGS5\", observation_start=start_date, observation_end=end_date)\n",
        "  #V19\n",
        "  df[\"DGS10\"] = fred.get_series(\"DGS10\", observation_start=start_date, observation_end=end_date)\n",
        "  #V20\n",
        "  df[\"DAAA\"] = fred.get_series(\"DAAA\", observation_start=start_date, observation_end=end_date)\n",
        "  #V21\n",
        "  df[\"DBAA\"] = fred.get_series(\"DBAA\", observation_start=start_date, observation_end=end_date)\n",
        "\n",
        "  #V22\n",
        "  df[\"TE1\"] = df[\"DGS10\"] - df[\"DTB4WK\"]\n",
        "  #V23\n",
        "  df[\"TE2\"] = df[\"DGS10\"] - df[\"DTB3\"]\n",
        "  #V24\n",
        "  df[\"TE3\"] = df[\"DGS10\"] - df[\"DTB6\"]\n",
        "\n",
        "  #V25\n",
        "  df[\"TE5\"] = df[\"DTB3\"] - df[\"DTB4WK\"]\n",
        "  #V26\n",
        "  df[\"TE6\"] = df[\"DTB6\"] - df[\"DTB4WK\"]\n",
        "\n",
        "  #V27\n",
        "  df[\"DE1\"] = df[\"DBAA\"] - df[\"DAAA\"] #Verify this - I believe there is a type in the paper (\"BAAA\")\n",
        "  #V28\n",
        "  df[\"DE2\"] = df[\"DBAA\"] - df[\"DGS10\"]\n",
        "\n",
        "  #V29\n",
        "  df[\"DE4\"] = df[\"DBAA\"] - df[\"DTB6\"]\n",
        "  #V30\n",
        "  df[\"DE5\"] = df[\"DBAA\"] - df[\"DTB3\"]\n",
        "  #V31\n",
        "  df[\"DE6\"] = df[\"DBAA\"] - df[\"DTB4WK\"]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2zZOILG7FK4"
      },
      "source": [
        "ABOVE THIS LINE, make sure the label match the dataset, rather than the table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J-9x4kr21vC"
      },
      "source": [
        "*V32 to V34*  \n",
        "These values are not the same as the ones on their dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vcqFPFMz37YA"
      },
      "outputs": [],
      "source": [
        "for index, df in data.items():\n",
        "  df[\"CTB3M\"] = fred.get_series('DGS3MO', observation_start=start_date, observation_end=end_date, units=\"pch\")\n",
        "  df[\"CTB6M\"] = fred.get_series('DGS6MO', observation_start=start_date, observation_end=end_date, units=\"pch\")\n",
        "  df[\"CTB1Y\"] = fred.get_series('DGS1', observation_start=start_date, observation_end=end_date, units=\"pch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbGQOGRH6dzc"
      },
      "source": [
        "*V35 to V44*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmO07Oba7pDj",
        "outputId": "d0448b96-5093-4c09-9ba6-1dbeb3a25813"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2009-12-31    0.29\n",
            "2010-01-01     NaN\n",
            "2010-01-04    1.14\n",
            "2010-01-05    0.22\n",
            "2010-01-06    0.87\n",
            "              ... \n",
            "2012-11-26    0.04\n",
            "2012-11-27   -0.85\n",
            "2012-11-28   -1.07\n",
            "2012-11-29    2.21\n",
            "2012-11-30    0.37\n",
            "Length: 762, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "T1 = fred.get_series('DCOILBRENTEU', observation_start=go_back_n_days(start_date, 1), observation_end=end_date)\n",
        "T2 = T1.fillna(method='ffill').diff()\n",
        "T3 = T2.where(T1.notna())\n",
        "print(T3.iloc[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wYkdTbwE6qgM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_19723/689674126.py:4: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  T2 = T1.fillna(method='ffill').diff()\n",
            "/tmp/ipykernel_19723/689674126.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  T2 = T1.fillna(method='ffill').diff()\n",
            "/tmp/ipykernel_19723/689674126.py:4: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  T2 = T1.fillna(method='ffill').diff()\n",
            "/tmp/ipykernel_19723/689674126.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  T2 = T1.fillna(method='ffill').diff()\n",
            "/tmp/ipykernel_19723/689674126.py:4: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  T2 = T1.fillna(method='ffill').diff()\n",
            "/tmp/ipykernel_19723/689674126.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  T2 = T1.fillna(method='ffill').diff()\n",
            "/tmp/ipykernel_19723/689674126.py:4: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  T2 = T1.fillna(method='ffill').diff()\n",
            "/tmp/ipykernel_19723/689674126.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  T2 = T1.fillna(method='ffill').diff()\n",
            "/tmp/ipykernel_19723/689674126.py:4: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  T2 = T1.fillna(method='ffill').diff()\n",
            "/tmp/ipykernel_19723/689674126.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  T2 = T1.fillna(method='ffill').diff()\n"
          ]
        }
      ],
      "source": [
        "for index, df in data.items():\n",
        "  #V35 - Value differ slightly from dataset\n",
        "  T1 = fred.get_series('DCOILWTICO', observation_start=go_back_n_days(start_date, 1), observation_end=end_date)\n",
        "  T2 = T1.fillna(method='ffill').diff()\n",
        "  df[\"WIT-oil\"] = T2.where(T1.notna())\n",
        "\n",
        "  #V36 - Value differ slightly from dataset, in fact there is no \"Brent-oil\", just Brent - using fred instead of investing.com\n",
        "  T1 = fred.get_series('DCOILWTICO', observation_start=go_back_n_days(start_date, 1), observation_end=end_date)\n",
        "  T2 = T1.fillna(method='ffill').diff()\n",
        "  df[\"Brent-oil\"] = T2.where(T1.notna())\n",
        "\n",
        "  #V37 Skipping due to redudancy\n",
        "\n",
        "  #38\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZbkLzbmesAq3"
      },
      "outputs": [],
      "source": [
        "for index, df in data.items():\n",
        "  for i in range(1, 83-41):\n",
        "    df[f'Close-{i}'] = df['Close']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDxjRtqZoV5R",
        "outputId": "18c630f9-35ea-4c83-f9b6-9b5487365b98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "82\n"
          ]
        }
      ],
      "source": [
        "print(len(data['^DJI'].columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9Ay7piZV14t"
      },
      "source": [
        "**Primative Data Save**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-gPhWgVPWVWl"
      },
      "outputs": [],
      "source": [
        "datasetName = \"tenTest\"\n",
        "datasetDir = \"./Datasets\"\n",
        "\n",
        "os.makedirs(datasetDir, exist_ok=True)\n",
        "datasetPath = join(datasetDir, datasetName) + \".pkl\"\n",
        "\n",
        "\n",
        "with open(datasetPath, 'wb') as file:\n",
        "    pickle.dump(data, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Der8STJzGL-r"
      },
      "source": [
        "**Pre-Processing Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iTTWDMy5ILXX"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import scale\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "#IMPORT FROM dataset.py CNNpred-pytorch\n",
        "class WholeDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, target):\n",
        "        self.data = data\n",
        "        self.target = target\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.target.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #print(\"Accessing data at index\", idx, \"with shape:\", self.data[idx].shape)\n",
        "        return self.data[idx], self.target[idx]\n",
        "\n",
        "#IMPORT FROM dataset.py CNNpred-pytorch\n",
        "def generate_batches(dataset, #ONLY GENERATES TWO BATCHES\n",
        "                     batch_size,\n",
        "                     shuffle=True,\n",
        "                     drop_last=False,\n",
        "                     device=\"cpu\",\n",
        "                     n_workers=0):\n",
        "    dataloader = DataLoader(dataset=dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=shuffle,\n",
        "                            drop_last=drop_last,\n",
        "                            num_workers=n_workers,\n",
        "                            pin_memory=False)\n",
        "\n",
        "    for data, labels in dataloader:\n",
        "        #data = torch.unsqueeze(data, 1).float()\n",
        "        data = data.float()\n",
        "        labels = labels.float()\n",
        "        yield data.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "\n",
        "#Personally I think this should be applied to the data before it is split into\n",
        "#train, test, and val, but following CNNpred code, the test set losses seq_len\n",
        "#points with this method\n",
        "def create_windows(X, y, seq_len):\n",
        "  series = []\n",
        "  target = []\n",
        "  for i in range(len(X) - seq_len + 1):\n",
        "    series.append(X[i: i + seq_len])\n",
        "    target.append(y[i + seq_len - 1])\n",
        "\n",
        "  return np.array(series), np.array(target)\n",
        "\n",
        "\n",
        "def preprocess(data: dict,\n",
        "               Split_Date: str,\n",
        "               seq_len: int,\n",
        "               val_percent: float,\n",
        "               n_day_predict: int = 1,\n",
        "               StartTrim: int = 200) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "  \"\"\"\n",
        "    Parameters:\n",
        "  - data (dict): Python dictionary that contains a dataframe for each stock\n",
        "  - target (str): Stock that is being predicted, str must match data key name\n",
        "  - Split_Date (str): The date (YYYY-MM-DD) that separates the training data from the testing data.\n",
        "  - Seq_len (int): Time series length of the model input\n",
        "  - val (float): percent of training devoted to validation\n",
        "  - n_day_predict (int, optional): The number of days ahead to predict, default is 1.\n",
        "  - StartTrim (int, optional): The number of initial rows to trim from the data for processing, default is 200.\n",
        "\n",
        "  Returns:\n",
        "  - Tuple containing:\n",
        "    - X_train_np (np.ndarray): The feature matrix for the training set.\n",
        "    - y_train (dict): The target dict for the training set. Key for each stock.\n",
        "    - X_test_np (np.ndarray): The feature matrix for the testing set.\n",
        "    - y_test (dict): The target dict for the testing set. Key for each stock.\n",
        "    - X_val_np (np.ndarray): The feature matrix for the validation set.\n",
        "    - y_val (dict): The target dict for the validation set. Key for each stock.\n",
        "\n",
        "\n",
        "    TODO: Ability to make Validation set = training set\n",
        "  \"\"\"\n",
        "\n",
        "  X_train = []\n",
        "\n",
        "  X_test = []\n",
        "\n",
        "  X_val = []\n",
        "\n",
        "  y_train = {}\n",
        "\n",
        "  y_test = {}\n",
        "\n",
        "  y_val = {}\n",
        "\n",
        "  for index, df in data.items():\n",
        "\n",
        "\n",
        "    y = (df['Close'][n_day_predict:] / df['Close'][:-n_day_predict].values).astype(int)\n",
        "    y = y[StartTrim - 1:]\n",
        "    y = y[:-n_day_predict]\n",
        "\n",
        "    #Split data\n",
        "    y_train_i = y[y.index < Split_Date]\n",
        "\n",
        "    y_test_i =  y[y.index >= Split_Date]\n",
        "\n",
        "\n",
        "    #Validation split\n",
        "    y_val_i = y_train_i[int((1-val_percent) * len(y_train_i)) - seq_len:]\n",
        "    y_train_i = y_train_i[:int((1-val_percent) * len(y_train_i))]\n",
        "\n",
        "    y_train[index] = y_train_i\n",
        "\n",
        "    y_test[index] = y_test_i\n",
        "\n",
        "    y_val[index] = y_val_i\n",
        "\n",
        "\n",
        "    #Trim and fill\n",
        "    df_pr = df[StartTrim:] #dataframe processed, first trim entries, 200 because of 200 day moving avg\n",
        "    df_pr = df_pr.fillna(0)\n",
        "    df_pr = df_pr[:-n_day_predict]\n",
        "    df_pr.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "\n",
        "\n",
        "    #Split data\n",
        "    train = df_pr[df_pr.index < Split_Date]\n",
        "    test = df_pr[df_pr.index  >= Split_Date]\n",
        "\n",
        "    #Validation split\n",
        "    val = train[int((1-val_percent) * len(train)) - seq_len:]\n",
        "    train = train[:int((1-val_percent) * len(train))]\n",
        "\n",
        "\n",
        "    #Scale data and append\n",
        "    X_train.append(scale(train))\n",
        "    X_test.append(scale(test))\n",
        "    X_val.append(scale(val))\n",
        "\n",
        "  #Create Windows\n",
        "  for index, y_train_i in y_train.items():\n",
        "    X_train_np, y_train[index] = create_windows(np.transpose(np.array(X_train), (1, 0, 2)), np.array(y_train_i).flatten(), seq_len)\n",
        "\n",
        "  for index, y_test_i in y_test.items():\n",
        "    X_test_np,  y_test[index]  = create_windows(np.transpose(np.array(X_test), (1, 0, 2)), np.array(y_test_i).flatten(), seq_len)\n",
        "\n",
        "  for index, y_val_i in y_val.items():\n",
        "    X_val_np,   y_val[index]   = create_windows(np.transpose(np.array(X_val), (1, 0, 2)), np.array(y_val_i).flatten(), seq_len)\n",
        "  return X_train_np, y_train, X_test_np,  y_test, X_val_np, y_val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjt5xf-LPaoa"
      },
      "source": [
        "**Model Definitions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG1NNBYsPhKp"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Notes on Default CNN model\n",
        "Parameters:\n",
        "seq_len(How far back are we using of time series data)\n",
        "epoc\n",
        "dropout ratio\n",
        "\n",
        "Generate CNN ready data windows\n",
        "\n",
        "for n times (They use 40)\n",
        "\n",
        "\n",
        "CNN pred arch\n",
        "\n",
        "\n",
        "Layer 1\n",
        "Conv2d\n",
        "filters: 8\n",
        "stride: (1,1)\n",
        "activation: relu\n",
        "kernal: (1,1)\n",
        "data_format: Channel_last\n",
        "input shape: (# stocks, sequence length, # features)\n",
        "\n",
        "Layer 2\n",
        "Conv2d\n",
        "filters: 8\n",
        "stride: (1,1)\n",
        "activation: relu\n",
        "kernal: (# stocks, 3)\n",
        "\n",
        "MaxPool2d\n",
        "pool size (1, 2)\n",
        "\n",
        "Layer 3\n",
        "Conv2d\n",
        "filters: 8\n",
        "stride: (1,1)\n",
        "activation: relu\n",
        "kernal: (# stocks, 3)\n",
        "\n",
        "MaxPool2d\n",
        "pool size (1, 2)\n",
        "\n",
        "Dropout 0.1\n",
        "\n",
        "Dense\n",
        "1, activation sigmoid\n",
        "\n",
        "optimizer Adam, Loss= Mae, metrics Acc, F1\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PaRLDbaDXVf3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, number_filter, number_of_stocks, seq_len, number_feature, drop):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, number_filter, kernel_size=(1, 1))\n",
        "        self.conv2 = nn.Conv2d(number_filter, number_filter, kernel_size=(3, number_of_stocks))\n",
        "        self.pool1 = nn.MaxPool2d((1, 2))\n",
        "        self.conv3 = nn.Conv2d(number_filter, number_filter, kernel_size=(3, 1))\n",
        "        self.pool2 = nn.MaxPool2d((1, 2))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(drop)\n",
        "        self.fc = nn.Linear(number_filter * (number_of_stocks * seq_len) // 4, 1)  # Adjust the size accordingly, should be 104 x 1 for 5x60x82\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool2(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.sigmoid(self.fc(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCiFDpaiT6Gj"
      },
      "source": [
        "**Utility Functions** Might combine with preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Urt_zmA6A1i1"
      },
      "outputs": [],
      "source": [
        "#Use for deriving next model filename\n",
        "def next_file(file, path):\n",
        "  #Ensure dicectory exists\n",
        "  os.makedirs(path, exist_ok=True)\n",
        "\n",
        "  #Get root and extension\n",
        "  filename, ext = os.path.splitext(file)\n",
        "\n",
        "  #Get list of files\n",
        "  files = os.listdir(path)\n",
        "  num = []\n",
        "  for file in files:\n",
        "    if file.startswith(filename):\n",
        "      num.append(int(re.findall(r'\\d+', file.split('-')[-1])[0]))\n",
        "\n",
        "  #Find next iteration number\n",
        "  file_iteration_number = max(num) + 1 if len(num) else 1\n",
        "  return join(path, filename + \"-\" + str(file_iteration_number) + ext)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KoOV7hsvU22"
      },
      "source": [
        "**Engine Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfgYeFimveI5"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "CNNpred-pytorch imp\n",
        "-Wand init (Optional)\n",
        "\n",
        "\n",
        "-Get training file names\n",
        "\n",
        "\n",
        "-'Construct_data_warehouse'\n",
        "\n",
        "\n",
        "-'Transforrm_data_warehouse'\n",
        "\n",
        "\n",
        "-CNN data sequences\n",
        "\n",
        "\n",
        "-Create training, val, and test\n",
        "\n",
        "\n",
        "-Train model i times\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8G-F8sXnLAU7"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "General Architecture\n",
        "\n",
        "Define Model\n",
        "-Model Type and hyperparams\n",
        "-optimizer\n",
        "-loss function\n",
        "-scheduler\n",
        "\n",
        "preprocess data\n",
        "-Get a y_train y_val y_test for each stock\n",
        "\n",
        ">For each iter_num(Can be set to just 1)\n",
        "  Directory Management\n",
        "  -Get name of next model file name\n",
        "\n",
        "\n",
        "  >For each stock\n",
        "    Train\n",
        "    -Take model params, and train\n",
        "    -Record Acc, F1, Loss\n",
        "\n",
        "  Record results\n",
        "  -In an CSV in the results/<modelname>/, record the Loss, Acc, F1 for each Stock\n",
        "  -In a 'Name' column, specify filename. i.e {Variant}-{iteration}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzAp2PS4lsPF",
        "outputId": "e3f2deb9-8bca-412e-a0b1-ac6e77acfd13"
      },
      "outputs": [],
      "source": [
        "#Main.py script\n",
        "#Define Model and preprocessing params\n",
        "config_path = \"./Configs/crypto_test.yaml\" #TODO: Define config file, this will be done in command line\n",
        "with open(config_path, \"r\") as file:\n",
        "  config = yaml.safe_load(file)\n",
        "\n",
        "with open(config['preprocess']['dataset_path'], 'rb') as file:\n",
        "  data = pickle.load(file)\n",
        "\n",
        "\n",
        "preparams = {\n",
        "    \"data\": data,\n",
        "    \"n_day_predict\": config['preprocess']['n_day_predict'],\n",
        "    \"seq_len\": config['preprocess']['seq_len'],\n",
        "    \"val_percent\": config['preprocess']['val_percent'],\n",
        "    \"StartTrim\" : config['preprocess']['StartTrim'],\n",
        "    \"Split_Date\": str(config['preprocess']['Split_Date'])\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DotCxsDOe38",
        "outputId": "154a76fb-1fb7-4ce2-f927-32849dff679c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/drew/anaconda3/envs/CNNpredRep/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:247: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\n",
            "/home/drew/anaconda3/envs/CNNpredRep/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:247: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\n",
            "/home/drew/anaconda3/envs/CNNpredRep/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:247: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\n",
            "/home/drew/anaconda3/envs/CNNpredRep/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:247: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\n",
            "/home/drew/anaconda3/envs/CNNpredRep/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:247: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\n",
            "/home/drew/anaconda3/envs/CNNpredRep/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:247: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\n",
            "/home/drew/anaconda3/envs/CNNpredRep/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:247: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\n",
            "/home/drew/anaconda3/envs/CNNpredRep/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:247: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\n",
            "/home/drew/anaconda3/envs/CNNpredRep/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:247: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\n",
            "/home/drew/anaconda3/envs/CNNpredRep/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:247: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1515, 60, 5, 82)\n"
          ]
        }
      ],
      "source": [
        "#Main.py script\n",
        "#preprocess data\n",
        "\n",
        "X_train, y_train, X_test, y_test, X_val, y_val = preprocess(**preparams)\n",
        "\n",
        "#Dict of Datasets\n",
        "train_data = {}\n",
        "val_data = {}\n",
        "test_data = {}\n",
        "\n",
        "#Dict of batch generaters\n",
        "train_dataloader = {}\n",
        "\n",
        "#Create wholedatasets in dict for each stock\n",
        "#Create batch generator for training\n",
        "print(X_train.shape)\n",
        "\n",
        "X_val = X_val.transpose(0, 3, 1, 2) #TEMP\n",
        "X_test = X_test.transpose(0, 3, 1, 2) #TEMP\n",
        "\n",
        "X_train = X_train.transpose(0, 3, 1, 2) #TEMP\n",
        "for index, y_train_i in y_train.items():\n",
        "  train_data[index] = WholeDataset(X_train, y_train_i)\n",
        "  # train_dataloader[index] = generate_batches(train_data[index], config['train']['batch_size'], config['train']['num_workers']) #TODO: Replace 128 and 0 with .yaml settings\n",
        "  #Train_dataloader moved to training loop, as it needs to be re init every time\n",
        "\n",
        "for index, y_val_i in y_val.items():\n",
        "\n",
        "  val_data[index] = WholeDataset(X_val, y_val_i)\n",
        "\n",
        "for index, y_test_i in y_test.items():\n",
        "  test_data[index] = WholeDataset(X_test, y_test_i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "kdFnleVyd1AD"
      },
      "outputs": [],
      "source": [
        "class CNNModelOne(nn.Module):\n",
        "    def __init__(self, number_filter, number_of_stocks, seq_len, number_feature, drop, calculated_fc_layer_size):\n",
        "        super(CNNModelOne, self).__init__()\n",
        "\n",
        "        # Layer 1\n",
        "        #self.conv1 = nn.Conv2d(in_channels=number_feature, out_channels=number_filter[0], kernel_size=(1, 1))\n",
        "        self.conv1 = nn.Conv2d(in_channels=82, out_channels=8, kernel_size=(1, 1))\n",
        "        # Layer 2\n",
        "        #self.conv2 = nn.Conv2d(in_channels=number_filter[0], out_channels=number_filter[1], kernel_size=(1, 3))\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=(3, 5))\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 1))\n",
        "\n",
        "        # Layer 3\n",
        "        self.conv3 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=(3, 1))\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 1))\n",
        "\n",
        "        # Flatten and Dropout\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(drop)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(calculated_fc_layer_size, 1)  # Calculate based on output size after last pooling\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        torch.Size([128, 82, 60, 5])\n",
        "        torch.Size([128, 8, 60, 5])\n",
        "        torch.Size([128, 8, 58, 1])\n",
        "        torch.Size([128, 8, 29, 1])\n",
        "        torch.Size([128, 8, 27, 1])\n",
        "        torch.Size([128, 8, 13, 1])\n",
        "        torch.Size([128, 104])\n",
        "        torch.Size([128, 104])\n",
        "        \"\"\"\n",
        "        #print(\"-----------------x-x-xxx--------------\")\n",
        "        #print(x.shape) # torch.Size([128, 82, 60, 5])\n",
        "        x = F.relu(self.conv1(x))\n",
        "        #print(x.shape) # torch.Size([128, 8, 60, 5])\n",
        "        x = F.relu(self.conv2(x))\n",
        "        #print(x.shape) # torch.Size([128, 8, 58, 1])\n",
        "        x = self.pool1(x)\n",
        "        #print(x.shape) # torch.Size([128, 8, 29, 1])\n",
        "        x = F.relu(self.conv3(x))\n",
        "        #print(x.shape) # torch.Size([128, 8, 27, 1])\n",
        "        x = self.pool2(x)\n",
        "        #print(x.shape) # torch.Size([128, 8, 13, 1])\n",
        "        x = self.flatten(x)\n",
        "        #print(x.shape) # torch.Size([128, 104])\n",
        "        x = self.dropout(x)\n",
        "        #print(x.shape) # torch.Size([128, 104])\n",
        "        x = torch.sigmoid(self.fc(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "KnpvMeGq4bMm"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "#Train.py script\n",
        "def validate(args, model, dataset):\n",
        "    model.eval()\n",
        "    loss_fcn = torch.nn.BCELoss()\n",
        "\n",
        "    data_dataloader = generate_batches(dataset, args.batch_size, n_workers=args.num_workers)\n",
        "\n",
        "\n",
        "    loss_list = []\n",
        "    pred_list = []\n",
        "    label_list = []\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_label in data_dataloader:\n",
        "            batch_logit = model(batch_data).view(-1)\n",
        "\n",
        "            loss = loss_fcn(batch_logit, batch_label)\n",
        "\n",
        "            pred = (batch_logit > 0.5).int()\n",
        "\n",
        "            pred_list.extend(pred)\n",
        "            label_list.extend(batch_label)\n",
        "\n",
        "            loss_list.append(loss.item())\n",
        "\n",
        "        loss_data = np.array(loss_list).mean()\n",
        "        acc = accuracy(pred_list, label_list)\n",
        "        f1 = f1_score(pred_list, label_list, average='macro')\n",
        "\n",
        "    return loss_data, acc, f1,\n",
        "def train(config, train_data):\n",
        "  #Init Model\n",
        "  #model_class = getattr(models, config['model']['type'])\n",
        "  #model = model_class(**config['model']['params'])\n",
        "\n",
        "  ####DEBUG DEBUG DEBUG\n",
        "  model = CNNModelOne(number_filter=[8,8,8], number_of_stocks=5, seq_len=60, number_feature=82, drop=0.1, calculated_fc_layer_size =104)\n",
        "  # print(\"-------Model----------\")\n",
        "  # print(model)\n",
        "  # print(model.conv3.kernel_size)\n",
        "  # print(model.conv3.padding)\n",
        "  # print(\"-------Model----------\")\n",
        "\n",
        "  #Init loss function\n",
        "  #loss_fcn = torch.nn.BCELoss()\n",
        "  loss_class = getattr(nn, config['loss_function']['type'])\n",
        "  loss_fcn = loss_class(**config['loss_function']['params'])\n",
        "\n",
        "  #Init Optimizer\n",
        "  #optimizer_class = getattr(optim, config['optimizer']['type'])\n",
        "  #optimizer = optimizer_class(model.parameters(), **{k: v for k, v in config['optimizer'].items() if k != 'type'})\n",
        "\n",
        "\n",
        "  ###DEBUG DEBUG DEBUG\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=128)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #Init Scheduler\n",
        "  #scheduler_class = getattr(optim.lr_scheduler, config['scheduler']['type'])\n",
        "  #scheduler = scheduler_class(optimizer, **{k: v for k, v in config['scheduler'].items() if k != 'type'})\n",
        "\n",
        "  ###DEBUG DEBUG DEBUG\n",
        "  scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, verbose=True, eps=1e-8)\n",
        "  for epoch in range(config['train']['epoch']):\n",
        "    model.eval()\n",
        "    loss_list = []\n",
        "    pred_list = []\n",
        "    label_list = []\n",
        "    #train_dataloader\n",
        "    train_dataloader = generate_batches(train_data, config['train']['batch_size'], config['train']['num_workers'])\n",
        "\n",
        "    for batch_data, batch_label in train_dataloader:\n",
        "      # print(\"-------bd----------\")\n",
        "      # print(batch_data.shape)\n",
        "      # print(\"-------------------\")\n",
        "      batch_logit = model(batch_data).view(-1)\n",
        "\n",
        "      loss = loss_fcn(batch_logit, batch_label)\n",
        "\n",
        "      # print(\"Batch_logit\")\n",
        "      # print(batch_logit)\n",
        "\n",
        "      pred = (batch_logit > 0.5).int()\n",
        "      # print(\"Pred\")\n",
        "      # print(pred)\n",
        "\n",
        "      pred_list.extend(pred)\n",
        "      label_list.extend(batch_label)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      parameters = list(model.parameters())\n",
        "      optimizer.step()\n",
        "      loss_list.append(loss.item())\n",
        "\n",
        "    loss_data = np.array(loss_list).mean()\n",
        "\n",
        "    train_acc = accuracy(pred_list, label_list)\n",
        "    # print(\"-------acc----------\")\n",
        "    # print(train_acc)\n",
        "    # print(pred_list)\n",
        "    # print(label_list)\n",
        "    # print(\"-------acc----------\")\n",
        "\n",
        "    train_f1 = f1_score(pred_list, label_list, average='macro')\n",
        "\n",
        "    print(\"Epoch {:05d}\\n\"\n",
        "          \"Train: loss: {:.4f} | accuracy: {:.4f} | f-acore: {:.4f}\"\n",
        "          .format(epoch + 1, loss_data, train_acc, train_f1))\n",
        "    #loss_data = float(loss_data.item())\n",
        "\n",
        "    loss_data = torch.tensor(loss_data)\n",
        "    scheduler.step(loss_data)\n",
        "\n",
        "    # if (epoch + 1) % 1 == 0:\n",
        "    #   val_loss, val_acc, val_f1 = validate(args, model, val_dataset)\n",
        "    #   print(\"Validation:  loss: {:.4f} | accuracy: {:.4f} | f1: {:.4f}\"\n",
        "    #         .format(val_loss, val_acc, val_f1))\n",
        "    #   # choosing best model according to best validation accuracy\n",
        "    #   if best_f1 < val_f1:\n",
        "    #       best_f1 = val_f1\n",
        "    #       cur_step = 0\n",
        "    #       torch.save(model, filepath)\n",
        "\n",
        "      # else:\n",
        "      #     cur_step += 1\n",
        "      #     if cur_step == config['train']['patience']:\n",
        "      #         break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5_m91VyEo3Ib",
        "outputId": "5a116a3a-4e07-46fe-a128-76b8c217b718"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/drew/anaconda3/envs/CNNpredRep/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 00001\n",
            "Train: loss: 0.6988 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00002\n",
            "Train: loss: 0.6986 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00003\n",
            "Train: loss: 0.6983 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00004\n",
            "Train: loss: 0.6981 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00005\n",
            "Train: loss: 0.6979 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00006\n",
            "Train: loss: 0.6977 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00007\n",
            "Train: loss: 0.6975 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00008\n",
            "Train: loss: 0.6973 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00009\n",
            "Train: loss: 0.6971 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00010\n",
            "Train: loss: 0.6970 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00011\n",
            "Train: loss: 0.6968 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00012\n",
            "Train: loss: 0.6967 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00013\n",
            "Train: loss: 0.6965 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00014\n",
            "Train: loss: 0.6964 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00015\n",
            "Train: loss: 0.6963 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00016\n",
            "Train: loss: 0.6962 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00017\n",
            "Train: loss: 0.6961 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00018\n",
            "Train: loss: 0.6960 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00019\n",
            "Train: loss: 0.6959 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00020\n",
            "Train: loss: 0.6958 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00021\n",
            "Train: loss: 0.6957 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00022\n",
            "Train: loss: 0.6957 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00023\n",
            "Train: loss: 0.6956 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00024\n",
            "Train: loss: 0.6955 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00025\n",
            "Train: loss: 0.6954 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00026\n",
            "Train: loss: 0.6954 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00027\n",
            "Train: loss: 0.6953 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00028\n",
            "Train: loss: 0.6953 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00029\n",
            "Train: loss: 0.6952 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00030\n",
            "Train: loss: 0.6951 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00031\n",
            "Train: loss: 0.6951 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00032\n",
            "Train: loss: 0.6950 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00033\n",
            "Train: loss: 0.6950 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00034\n",
            "Train: loss: 0.6949 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00035\n",
            "Train: loss: 0.6949 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00036\n",
            "Train: loss: 0.6948 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00037\n",
            "Train: loss: 0.6948 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00038\n",
            "Train: loss: 0.6947 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00039\n",
            "Train: loss: 0.6947 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00040\n",
            "Train: loss: 0.6946 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00041\n",
            "Train: loss: 0.6946 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00042\n",
            "Train: loss: 0.6946 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00043\n",
            "Train: loss: 0.6945 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00044\n",
            "Train: loss: 0.6945 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00045\n",
            "Train: loss: 0.6944 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00046\n",
            "Train: loss: 0.6944 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00047\n",
            "Train: loss: 0.6944 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00048\n",
            "Train: loss: 0.6943 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00049\n",
            "Train: loss: 0.6943 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00050\n",
            "Train: loss: 0.6943 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00051\n",
            "Train: loss: 0.6942 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00052\n",
            "Train: loss: 0.6942 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00053\n",
            "Train: loss: 0.6942 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00054\n",
            "Train: loss: 0.6941 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00055\n",
            "Train: loss: 0.6941 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00056\n",
            "Train: loss: 0.6941 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00057\n",
            "Train: loss: 0.6941 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00058\n",
            "Train: loss: 0.6940 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00059\n",
            "Train: loss: 0.6940 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00060\n",
            "Train: loss: 0.6940 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00061\n",
            "Train: loss: 0.6940 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00062\n",
            "Train: loss: 0.6939 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00063\n",
            "Train: loss: 0.6939 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00064\n",
            "Train: loss: 0.6939 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00065\n",
            "Train: loss: 0.6939 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00066\n",
            "Train: loss: 0.6938 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00067\n",
            "Train: loss: 0.6938 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00068\n",
            "Train: loss: 0.6938 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00069\n",
            "Train: loss: 0.6938 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00070\n",
            "Train: loss: 0.6938 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00071\n",
            "Train: loss: 0.6937 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00072\n",
            "Train: loss: 0.6937 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00073\n",
            "Train: loss: 0.6937 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00074\n",
            "Train: loss: 0.6937 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00075\n",
            "Train: loss: 0.6937 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00076\n",
            "Train: loss: 0.6937 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00077\n",
            "Train: loss: 0.6936 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00078\n",
            "Train: loss: 0.6936 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00079\n",
            "Train: loss: 0.6936 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00080\n",
            "Train: loss: 0.6936 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00081\n",
            "Train: loss: 0.6936 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00082\n",
            "Train: loss: 0.6936 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00083\n",
            "Train: loss: 0.6936 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00084\n",
            "Train: loss: 0.6935 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00085\n",
            "Train: loss: 0.6935 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00086\n",
            "Train: loss: 0.6935 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00087\n",
            "Train: loss: 0.6935 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00088\n",
            "Train: loss: 0.6935 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00089\n",
            "Train: loss: 0.6935 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00090\n",
            "Train: loss: 0.6935 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00091\n",
            "Train: loss: 0.6935 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00092\n",
            "Train: loss: 0.6935 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00093\n",
            "Train: loss: 0.6934 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00094\n",
            "Train: loss: 0.6934 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00095\n",
            "Train: loss: 0.6934 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00096\n",
            "Train: loss: 0.6934 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00097\n",
            "Train: loss: 0.6934 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00098\n",
            "Train: loss: 0.6934 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00099\n",
            "Train: loss: 0.6934 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00100\n",
            "Train: loss: 0.6934 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00101\n",
            "Train: loss: 0.6934 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00102\n",
            "Train: loss: 0.6934 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00103\n",
            "Train: loss: 0.6934 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00104\n",
            "Train: loss: 0.6934 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00105\n",
            "Train: loss: 0.6934 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00106\n",
            "Train: loss: 0.6933 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00107\n",
            "Train: loss: 0.6933 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00108\n",
            "Train: loss: 0.6933 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00109\n",
            "Train: loss: 0.6933 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00110\n",
            "Train: loss: 0.6933 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00111\n",
            "Train: loss: 0.6933 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00112\n",
            "Train: loss: 0.6933 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00113\n",
            "Train: loss: 0.6933 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00114\n",
            "Train: loss: 0.6933 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00115\n",
            "Train: loss: 0.6933 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00116\n",
            "Train: loss: 0.6933 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00117\n",
            "Train: loss: 0.6933 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00118\n",
            "Train: loss: 0.6933 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00119\n",
            "Train: loss: 0.6933 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00120\n",
            "Train: loss: 0.6933 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00121\n",
            "Train: loss: 0.6933 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00122\n",
            "Train: loss: 0.6933 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00123\n",
            "Train: loss: 0.6933 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00124\n",
            "Train: loss: 0.6933 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00125\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00126\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00127\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00128\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00129\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00130\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00131\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00132\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00133\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00134\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00135\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00136\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00137\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00138\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00139\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00140\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00141\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00142\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00143\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00144\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00145\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00146\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00147\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00148\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00149\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00150\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00151\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00152\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00153\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00154\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00155\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00156\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00157\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00158\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00159\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00160\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00161\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00162\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00163\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00164\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00165\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00166\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00167\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00168\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00169\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00170\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00171\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00172\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00173\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00174\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00175\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00176\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00177\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00178\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00179\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00180\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00181\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00182\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00183\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00184\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00185\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00186\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00187\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00188\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00189\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00190\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00191\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00192\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00193\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00194\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00195\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00196\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00197\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00198\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00199\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n",
            "Epoch 00200\n",
            "Train: loss: 0.6932 | accuracy: 0.4825 | f-acore: 0.3255\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/drew/anaconda3/envs/CNNpredRep/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 00001\n",
            "Train: loss: 0.6926 | accuracy: 0.5274 | f-acore: 0.5265\n",
            "Epoch 00002\n",
            "Train: loss: 0.6925 | accuracy: 0.5281 | f-acore: 0.5280\n",
            "Epoch 00003\n",
            "Train: loss: 0.6925 | accuracy: 0.5261 | f-acore: 0.5254\n",
            "Epoch 00004\n",
            "Train: loss: 0.6925 | accuracy: 0.5182 | f-acore: 0.5148\n",
            "Epoch 00005\n",
            "Train: loss: 0.6925 | accuracy: 0.5168 | f-acore: 0.5067\n",
            "Epoch 00006\n",
            "Train: loss: 0.6925 | accuracy: 0.5234 | f-acore: 0.5062\n",
            "Epoch 00007\n",
            "Train: loss: 0.6925 | accuracy: 0.5195 | f-acore: 0.4925\n",
            "Epoch 00008\n",
            "Train: loss: 0.6925 | accuracy: 0.5116 | f-acore: 0.4719\n",
            "Epoch 00009\n",
            "Train: loss: 0.6925 | accuracy: 0.5188 | f-acore: 0.4651\n",
            "Epoch 00010\n",
            "Train: loss: 0.6925 | accuracy: 0.5215 | f-acore: 0.4547\n",
            "Epoch 00011\n",
            "Train: loss: 0.6925 | accuracy: 0.5248 | f-acore: 0.4467\n",
            "Epoch 00012\n",
            "Train: loss: 0.6925 | accuracy: 0.5208 | f-acore: 0.4306\n",
            "Epoch 00013\n",
            "Train: loss: 0.6926 | accuracy: 0.5188 | f-acore: 0.4111\n",
            "Epoch 00014\n",
            "Train: loss: 0.6926 | accuracy: 0.5162 | f-acore: 0.4003\n",
            "Epoch 00015\n",
            "Train: loss: 0.6926 | accuracy: 0.5215 | f-acore: 0.3916\n",
            "Epoch 00016\n",
            "Train: loss: 0.6926 | accuracy: 0.5195 | f-acore: 0.3788\n",
            "Epoch 00017\n",
            "Train: loss: 0.6926 | accuracy: 0.5188 | f-acore: 0.3723\n",
            "Epoch 00018\n",
            "Train: loss: 0.6926 | accuracy: 0.5175 | f-acore: 0.3652\n",
            "Epoch 00019\n",
            "Train: loss: 0.6926 | accuracy: 0.5168 | f-acore: 0.3561\n",
            "Epoch 00020\n",
            "Train: loss: 0.6926 | accuracy: 0.5182 | f-acore: 0.3521\n",
            "Epoch 00021\n",
            "Train: loss: 0.6926 | accuracy: 0.5175 | f-acore: 0.3483\n",
            "Epoch 00022\n",
            "Train: loss: 0.6926 | accuracy: 0.5175 | f-acore: 0.3447\n",
            "Epoch 00023\n",
            "Train: loss: 0.6926 | accuracy: 0.5168 | f-acore: 0.3420\n",
            "Epoch 00024\n",
            "Train: loss: 0.6926 | accuracy: 0.5162 | f-acore: 0.3404\n",
            "Epoch 00025\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00026\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00027\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00028\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00029\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00030\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00031\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00032\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00033\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00034\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00035\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00036\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00037\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00038\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00039\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00040\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00041\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00042\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00043\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00044\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00045\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00046\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00047\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00048\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00049\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00050\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00051\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00052\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00053\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00054\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00055\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00056\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00057\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00058\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00059\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00060\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00061\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00062\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00063\n",
            "Train: loss: 0.6927 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00064\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00065\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00066\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00067\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00068\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00069\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00070\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00071\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00072\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00073\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00074\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00075\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00076\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00077\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00078\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00079\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00080\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00081\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00082\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00083\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00084\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00085\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00086\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00087\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00088\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00089\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00090\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00091\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00092\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00093\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00094\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00095\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00096\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00097\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00098\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00099\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00100\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00101\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00102\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00103\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00104\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00105\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00106\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00107\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00108\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00109\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00110\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00111\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00112\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00113\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00114\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00115\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00116\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00117\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00118\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00119\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00120\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00121\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00122\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00123\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00124\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00125\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00126\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00127\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00128\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00129\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00130\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00131\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00132\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00133\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00134\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00135\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00136\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00137\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00138\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00139\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00140\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00141\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00142\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00143\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00144\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00145\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00146\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00147\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00148\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00149\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00150\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00151\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00152\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00153\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00154\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00155\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00156\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00157\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00158\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00159\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00160\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00161\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00162\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00163\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00164\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00165\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00166\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00167\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00168\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00169\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00170\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00171\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00172\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00173\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00174\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00175\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00176\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00177\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00178\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00179\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00180\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00181\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00182\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00183\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00184\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00185\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00186\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00187\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00188\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00189\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00190\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00191\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00192\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00193\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00194\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00195\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00196\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00197\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00198\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00199\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n",
            "Epoch 00200\n",
            "Train: loss: 0.6928 | accuracy: 0.5168 | f-acore: 0.3407\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/drew/anaconda3/envs/CNNpredRep/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 00001\n",
            "Train: loss: 0.6932 | accuracy: 0.5056 | f-acore: 0.5027\n",
            "Epoch 00002\n",
            "Train: loss: 0.6931 | accuracy: 0.5083 | f-acore: 0.5074\n",
            "Epoch 00003\n",
            "Train: loss: 0.6931 | accuracy: 0.5096 | f-acore: 0.5086\n",
            "Epoch 00004\n",
            "Train: loss: 0.6931 | accuracy: 0.5056 | f-acore: 0.5044\n",
            "Epoch 00005\n",
            "Train: loss: 0.6931 | accuracy: 0.5083 | f-acore: 0.5074\n",
            "Epoch 00006\n",
            "Train: loss: 0.6931 | accuracy: 0.5069 | f-acore: 0.5063\n",
            "Epoch 00007\n",
            "Train: loss: 0.6931 | accuracy: 0.5096 | f-acore: 0.5089\n",
            "Epoch 00008\n",
            "Train: loss: 0.6931 | accuracy: 0.5089 | f-acore: 0.5083\n",
            "Epoch 00009\n",
            "Train: loss: 0.6931 | accuracy: 0.5142 | f-acore: 0.5137\n",
            "Epoch 00010\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.5163\n",
            "Epoch 00011\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.5164\n",
            "Epoch 00012\n",
            "Train: loss: 0.6931 | accuracy: 0.5122 | f-acore: 0.5119\n",
            "Epoch 00013\n",
            "Train: loss: 0.6931 | accuracy: 0.5083 | f-acore: 0.5079\n",
            "Epoch 00014\n",
            "Train: loss: 0.6931 | accuracy: 0.5076 | f-acore: 0.5072\n",
            "Epoch 00015\n",
            "Train: loss: 0.6931 | accuracy: 0.5076 | f-acore: 0.5073\n",
            "Epoch 00016\n",
            "Train: loss: 0.6931 | accuracy: 0.5050 | f-acore: 0.5047\n",
            "Epoch 00017\n",
            "Train: loss: 0.6931 | accuracy: 0.5036 | f-acore: 0.5034\n",
            "Epoch 00018\n",
            "Train: loss: 0.6931 | accuracy: 0.5010 | f-acore: 0.5007\n",
            "Epoch 00019\n",
            "Train: loss: 0.6931 | accuracy: 0.5010 | f-acore: 0.5008\n",
            "Epoch 00020\n",
            "Train: loss: 0.6931 | accuracy: 0.4977 | f-acore: 0.4976\n",
            "Epoch 00021\n",
            "Train: loss: 0.6932 | accuracy: 0.4950 | f-acore: 0.4949\n",
            "Epoch 00022\n",
            "Train: loss: 0.6932 | accuracy: 0.4924 | f-acore: 0.4923\n",
            "Epoch 00023\n",
            "Train: loss: 0.6932 | accuracy: 0.4970 | f-acore: 0.4970\n",
            "Epoch 00024\n",
            "Train: loss: 0.6932 | accuracy: 0.4970 | f-acore: 0.4970\n",
            "Epoch 00025\n",
            "Train: loss: 0.6932 | accuracy: 0.4977 | f-acore: 0.4976\n",
            "Epoch 00026\n",
            "Train: loss: 0.6932 | accuracy: 0.4977 | f-acore: 0.4977\n",
            "Epoch 00027\n",
            "Train: loss: 0.6932 | accuracy: 0.4970 | f-acore: 0.4970\n",
            "Epoch 00028\n",
            "Train: loss: 0.6932 | accuracy: 0.4983 | f-acore: 0.4983\n",
            "Epoch 00029\n",
            "Train: loss: 0.6932 | accuracy: 0.4997 | f-acore: 0.4997\n",
            "Epoch 00030\n",
            "Train: loss: 0.6932 | accuracy: 0.4997 | f-acore: 0.4997\n",
            "Epoch 00031\n",
            "Train: loss: 0.6932 | accuracy: 0.4983 | f-acore: 0.4983\n",
            "Epoch 00032\n",
            "Train: loss: 0.6932 | accuracy: 0.4990 | f-acore: 0.4990\n",
            "Epoch 00033\n",
            "Train: loss: 0.6932 | accuracy: 0.4990 | f-acore: 0.4990\n",
            "Epoch 00034\n",
            "Train: loss: 0.6932 | accuracy: 0.4983 | f-acore: 0.4983\n",
            "Epoch 00035\n",
            "Train: loss: 0.6932 | accuracy: 0.4931 | f-acore: 0.4931\n",
            "Epoch 00036\n",
            "Train: loss: 0.6932 | accuracy: 0.4957 | f-acore: 0.4957\n",
            "Epoch 00037\n",
            "Train: loss: 0.6932 | accuracy: 0.4970 | f-acore: 0.4970\n",
            "Epoch 00038\n",
            "Train: loss: 0.6932 | accuracy: 0.4997 | f-acore: 0.4997\n",
            "Epoch 00039\n",
            "Train: loss: 0.6932 | accuracy: 0.4990 | f-acore: 0.4990\n",
            "Epoch 00040\n",
            "Train: loss: 0.6932 | accuracy: 0.4977 | f-acore: 0.4976\n",
            "Epoch 00041\n",
            "Train: loss: 0.6932 | accuracy: 0.4990 | f-acore: 0.4990\n",
            "Epoch 00042\n",
            "Train: loss: 0.6932 | accuracy: 0.4983 | f-acore: 0.4982\n",
            "Epoch 00043\n",
            "Train: loss: 0.6932 | accuracy: 0.4970 | f-acore: 0.4969\n",
            "Epoch 00044\n",
            "Train: loss: 0.6932 | accuracy: 0.5030 | f-acore: 0.5027\n",
            "Epoch 00045\n",
            "Train: loss: 0.6932 | accuracy: 0.5030 | f-acore: 0.5027\n",
            "Epoch 00046\n",
            "Train: loss: 0.6932 | accuracy: 0.5023 | f-acore: 0.5021\n",
            "Epoch 00047\n",
            "Train: loss: 0.6932 | accuracy: 0.5043 | f-acore: 0.5040\n",
            "Epoch 00048\n",
            "Train: loss: 0.6932 | accuracy: 0.5050 | f-acore: 0.5047\n",
            "Epoch 00049\n",
            "Train: loss: 0.6932 | accuracy: 0.5050 | f-acore: 0.5047\n",
            "Epoch 00050\n",
            "Train: loss: 0.6932 | accuracy: 0.5063 | f-acore: 0.5060\n",
            "Epoch 00051\n",
            "Train: loss: 0.6932 | accuracy: 0.5069 | f-acore: 0.5067\n",
            "Epoch 00052\n",
            "Train: loss: 0.6932 | accuracy: 0.5050 | f-acore: 0.5047\n",
            "Epoch 00053\n",
            "Train: loss: 0.6932 | accuracy: 0.5056 | f-acore: 0.5053\n",
            "Epoch 00054\n",
            "Train: loss: 0.6932 | accuracy: 0.5063 | f-acore: 0.5060\n",
            "Epoch 00055\n",
            "Train: loss: 0.6932 | accuracy: 0.5043 | f-acore: 0.5041\n",
            "Epoch 00056\n",
            "Train: loss: 0.6932 | accuracy: 0.5036 | f-acore: 0.5035\n",
            "Epoch 00057\n",
            "Train: loss: 0.6932 | accuracy: 0.5036 | f-acore: 0.5035\n",
            "Epoch 00058\n",
            "Train: loss: 0.6932 | accuracy: 0.5050 | f-acore: 0.5048\n",
            "Epoch 00059\n",
            "Train: loss: 0.6932 | accuracy: 0.5043 | f-acore: 0.5041\n",
            "Epoch 00060\n",
            "Train: loss: 0.6932 | accuracy: 0.5043 | f-acore: 0.5041\n",
            "Epoch 00061\n",
            "Train: loss: 0.6932 | accuracy: 0.5043 | f-acore: 0.5041\n",
            "Epoch 00062\n",
            "Train: loss: 0.6932 | accuracy: 0.5036 | f-acore: 0.5034\n",
            "Epoch 00063\n",
            "Train: loss: 0.6932 | accuracy: 0.5030 | f-acore: 0.5028\n",
            "Epoch 00064\n",
            "Train: loss: 0.6932 | accuracy: 0.5050 | f-acore: 0.5048\n",
            "Epoch 00065\n",
            "Train: loss: 0.6932 | accuracy: 0.5030 | f-acore: 0.5027\n",
            "Epoch 00066\n",
            "Train: loss: 0.6932 | accuracy: 0.5036 | f-acore: 0.5034\n",
            "Epoch 00067\n",
            "Train: loss: 0.6932 | accuracy: 0.5036 | f-acore: 0.5034\n",
            "Epoch 00068\n",
            "Train: loss: 0.6932 | accuracy: 0.5043 | f-acore: 0.5041\n",
            "Epoch 00069\n",
            "Train: loss: 0.6932 | accuracy: 0.5050 | f-acore: 0.5047\n",
            "Epoch 00070\n",
            "Train: loss: 0.6932 | accuracy: 0.5056 | f-acore: 0.5054\n",
            "Epoch 00071\n",
            "Train: loss: 0.6932 | accuracy: 0.5056 | f-acore: 0.5054\n",
            "Epoch 00072\n",
            "Train: loss: 0.6932 | accuracy: 0.5063 | f-acore: 0.5061\n",
            "Epoch 00073\n",
            "Train: loss: 0.6932 | accuracy: 0.5056 | f-acore: 0.5054\n",
            "Epoch 00074\n",
            "Train: loss: 0.6932 | accuracy: 0.5056 | f-acore: 0.5054\n",
            "Epoch 00075\n",
            "Train: loss: 0.6932 | accuracy: 0.5043 | f-acore: 0.5041\n",
            "Epoch 00076\n",
            "Train: loss: 0.6932 | accuracy: 0.5023 | f-acore: 0.5021\n",
            "Epoch 00077\n",
            "Train: loss: 0.6932 | accuracy: 0.5023 | f-acore: 0.5021\n",
            "Epoch 00078\n",
            "Train: loss: 0.6932 | accuracy: 0.5017 | f-acore: 0.5014\n",
            "Epoch 00079\n",
            "Train: loss: 0.6932 | accuracy: 0.5003 | f-acore: 0.5001\n",
            "Epoch 00080\n",
            "Train: loss: 0.6932 | accuracy: 0.4997 | f-acore: 0.4994\n",
            "Epoch 00081\n",
            "Train: loss: 0.6932 | accuracy: 0.4997 | f-acore: 0.4994\n",
            "Epoch 00082\n",
            "Train: loss: 0.6932 | accuracy: 0.4990 | f-acore: 0.4987\n",
            "Epoch 00083\n",
            "Train: loss: 0.6932 | accuracy: 0.4977 | f-acore: 0.4973\n",
            "Epoch 00084\n",
            "Train: loss: 0.6932 | accuracy: 0.4977 | f-acore: 0.4973\n",
            "Epoch 00085\n",
            "Train: loss: 0.6932 | accuracy: 0.4977 | f-acore: 0.4973\n",
            "Epoch 00086\n",
            "Train: loss: 0.6932 | accuracy: 0.4997 | f-acore: 0.4991\n",
            "Epoch 00087\n",
            "Train: loss: 0.6932 | accuracy: 0.4997 | f-acore: 0.4991\n",
            "Epoch 00088\n",
            "Train: loss: 0.6932 | accuracy: 0.5003 | f-acore: 0.4998\n",
            "Epoch 00089\n",
            "Train: loss: 0.6932 | accuracy: 0.5023 | f-acore: 0.5018\n",
            "Epoch 00090\n",
            "Train: loss: 0.6932 | accuracy: 0.5023 | f-acore: 0.5018\n",
            "Epoch 00091\n",
            "Train: loss: 0.6932 | accuracy: 0.5010 | f-acore: 0.5005\n",
            "Epoch 00092\n",
            "Train: loss: 0.6932 | accuracy: 0.5003 | f-acore: 0.4999\n",
            "Epoch 00093\n",
            "Train: loss: 0.6932 | accuracy: 0.4997 | f-acore: 0.4993\n",
            "Epoch 00094\n",
            "Train: loss: 0.6932 | accuracy: 0.4990 | f-acore: 0.4986\n",
            "Epoch 00095\n",
            "Train: loss: 0.6932 | accuracy: 0.4997 | f-acore: 0.4992\n",
            "Epoch 00096\n",
            "Train: loss: 0.6932 | accuracy: 0.4997 | f-acore: 0.4992\n",
            "Epoch 00097\n",
            "Train: loss: 0.6932 | accuracy: 0.4997 | f-acore: 0.4992\n",
            "Epoch 00098\n",
            "Train: loss: 0.6932 | accuracy: 0.5003 | f-acore: 0.4999\n",
            "Epoch 00099\n",
            "Train: loss: 0.6932 | accuracy: 0.5010 | f-acore: 0.5006\n",
            "Epoch 00100\n",
            "Train: loss: 0.6932 | accuracy: 0.5017 | f-acore: 0.5013\n",
            "Epoch 00101\n",
            "Train: loss: 0.6932 | accuracy: 0.5010 | f-acore: 0.5006\n",
            "Epoch 00102\n",
            "Train: loss: 0.6932 | accuracy: 0.4997 | f-acore: 0.4993\n",
            "Epoch 00103\n",
            "Train: loss: 0.6932 | accuracy: 0.5003 | f-acore: 0.5000\n",
            "Epoch 00104\n",
            "Train: loss: 0.6932 | accuracy: 0.4990 | f-acore: 0.4987\n",
            "Epoch 00105\n",
            "Train: loss: 0.6932 | accuracy: 0.4990 | f-acore: 0.4987\n",
            "Epoch 00106\n",
            "Train: loss: 0.6932 | accuracy: 0.4983 | f-acore: 0.4980\n",
            "Epoch 00107\n",
            "Train: loss: 0.6932 | accuracy: 0.4977 | f-acore: 0.4973\n",
            "Epoch 00108\n",
            "Train: loss: 0.6932 | accuracy: 0.4977 | f-acore: 0.4973\n",
            "Epoch 00109\n",
            "Train: loss: 0.6932 | accuracy: 0.4983 | f-acore: 0.4980\n",
            "Epoch 00110\n",
            "Train: loss: 0.6932 | accuracy: 0.4983 | f-acore: 0.4980\n",
            "Epoch 00111\n",
            "Train: loss: 0.6932 | accuracy: 0.4977 | f-acore: 0.4973\n",
            "Epoch 00112\n",
            "Train: loss: 0.6932 | accuracy: 0.4977 | f-acore: 0.4973\n",
            "Epoch 00113\n",
            "Train: loss: 0.6932 | accuracy: 0.4964 | f-acore: 0.4960\n",
            "Epoch 00114\n",
            "Train: loss: 0.6932 | accuracy: 0.4964 | f-acore: 0.4960\n",
            "Epoch 00115\n",
            "Train: loss: 0.6932 | accuracy: 0.4964 | f-acore: 0.4960\n",
            "Epoch 00116\n",
            "Train: loss: 0.6932 | accuracy: 0.4964 | f-acore: 0.4960\n",
            "Epoch 00117\n",
            "Train: loss: 0.6932 | accuracy: 0.4957 | f-acore: 0.4954\n",
            "Epoch 00118\n",
            "Train: loss: 0.6932 | accuracy: 0.4957 | f-acore: 0.4954\n",
            "Epoch 00119\n",
            "Train: loss: 0.6932 | accuracy: 0.4950 | f-acore: 0.4948\n",
            "Epoch 00120\n",
            "Train: loss: 0.6932 | accuracy: 0.4950 | f-acore: 0.4948\n",
            "Epoch 00121\n",
            "Train: loss: 0.6932 | accuracy: 0.4957 | f-acore: 0.4954\n",
            "Epoch 00122\n",
            "Train: loss: 0.6932 | accuracy: 0.4957 | f-acore: 0.4954\n",
            "Epoch 00123\n",
            "Train: loss: 0.6932 | accuracy: 0.4964 | f-acore: 0.4961\n",
            "Epoch 00124\n",
            "Train: loss: 0.6932 | accuracy: 0.4964 | f-acore: 0.4961\n",
            "Epoch 00125\n",
            "Train: loss: 0.6932 | accuracy: 0.4964 | f-acore: 0.4961\n",
            "Epoch 00126\n",
            "Train: loss: 0.6932 | accuracy: 0.4957 | f-acore: 0.4954\n",
            "Epoch 00127\n",
            "Train: loss: 0.6932 | accuracy: 0.4964 | f-acore: 0.4961\n",
            "Epoch 00128\n",
            "Train: loss: 0.6932 | accuracy: 0.4957 | f-acore: 0.4954\n",
            "Epoch 00129\n",
            "Train: loss: 0.6932 | accuracy: 0.4964 | f-acore: 0.4961\n",
            "Epoch 00130\n",
            "Train: loss: 0.6932 | accuracy: 0.4970 | f-acore: 0.4968\n",
            "Epoch 00131\n",
            "Train: loss: 0.6932 | accuracy: 0.4970 | f-acore: 0.4968\n",
            "Epoch 00132\n",
            "Train: loss: 0.6932 | accuracy: 0.4970 | f-acore: 0.4968\n",
            "Epoch 00133\n",
            "Train: loss: 0.6932 | accuracy: 0.4964 | f-acore: 0.4961\n",
            "Epoch 00134\n",
            "Train: loss: 0.6932 | accuracy: 0.4964 | f-acore: 0.4961\n",
            "Epoch 00135\n",
            "Train: loss: 0.6932 | accuracy: 0.4964 | f-acore: 0.4961\n",
            "Epoch 00136\n",
            "Train: loss: 0.6932 | accuracy: 0.4964 | f-acore: 0.4961\n",
            "Epoch 00137\n",
            "Train: loss: 0.6931 | accuracy: 0.4964 | f-acore: 0.4961\n",
            "Epoch 00138\n",
            "Train: loss: 0.6932 | accuracy: 0.4964 | f-acore: 0.4961\n",
            "Epoch 00139\n",
            "Train: loss: 0.6932 | accuracy: 0.4964 | f-acore: 0.4961\n",
            "Epoch 00140\n",
            "Train: loss: 0.6932 | accuracy: 0.4964 | f-acore: 0.4961\n",
            "Epoch 00141\n",
            "Train: loss: 0.6932 | accuracy: 0.4957 | f-acore: 0.4954\n",
            "Epoch 00142\n",
            "Train: loss: 0.6932 | accuracy: 0.4957 | f-acore: 0.4954\n",
            "Epoch 00143\n",
            "Train: loss: 0.6931 | accuracy: 0.4957 | f-acore: 0.4954\n",
            "Epoch 00144\n",
            "Train: loss: 0.6932 | accuracy: 0.4957 | f-acore: 0.4954\n",
            "Epoch 00145\n",
            "Train: loss: 0.6932 | accuracy: 0.4957 | f-acore: 0.4954\n",
            "Epoch 00146\n",
            "Train: loss: 0.6932 | accuracy: 0.4964 | f-acore: 0.4961\n",
            "Epoch 00147\n",
            "Train: loss: 0.6931 | accuracy: 0.4964 | f-acore: 0.4961\n",
            "Epoch 00148\n",
            "Train: loss: 0.6931 | accuracy: 0.4964 | f-acore: 0.4961\n",
            "Epoch 00149\n",
            "Train: loss: 0.6931 | accuracy: 0.4964 | f-acore: 0.4961\n",
            "Epoch 00150\n",
            "Train: loss: 0.6931 | accuracy: 0.4964 | f-acore: 0.4961\n",
            "Epoch 00151\n",
            "Train: loss: 0.6931 | accuracy: 0.4964 | f-acore: 0.4961\n",
            "Epoch 00152\n",
            "Train: loss: 0.6931 | accuracy: 0.4964 | f-acore: 0.4961\n",
            "Epoch 00153\n",
            "Train: loss: 0.6931 | accuracy: 0.4957 | f-acore: 0.4954\n",
            "Epoch 00154\n",
            "Train: loss: 0.6931 | accuracy: 0.4950 | f-acore: 0.4948\n",
            "Epoch 00155\n",
            "Train: loss: 0.6931 | accuracy: 0.4950 | f-acore: 0.4948\n",
            "Epoch 00156\n",
            "Train: loss: 0.6931 | accuracy: 0.4944 | f-acore: 0.4941\n",
            "Epoch 00157\n",
            "Train: loss: 0.6931 | accuracy: 0.4944 | f-acore: 0.4941\n",
            "Epoch 00158\n",
            "Train: loss: 0.6931 | accuracy: 0.4944 | f-acore: 0.4941\n",
            "Epoch 00159\n",
            "Train: loss: 0.6931 | accuracy: 0.4944 | f-acore: 0.4941\n",
            "Epoch 00160\n",
            "Train: loss: 0.6931 | accuracy: 0.4944 | f-acore: 0.4941\n",
            "Epoch 00161\n",
            "Train: loss: 0.6931 | accuracy: 0.4944 | f-acore: 0.4941\n",
            "Epoch 00162\n",
            "Train: loss: 0.6931 | accuracy: 0.4944 | f-acore: 0.4941\n",
            "Epoch 00163\n",
            "Train: loss: 0.6931 | accuracy: 0.4944 | f-acore: 0.4941\n",
            "Epoch 00164\n",
            "Train: loss: 0.6931 | accuracy: 0.4944 | f-acore: 0.4941\n",
            "Epoch 00165\n",
            "Train: loss: 0.6931 | accuracy: 0.4944 | f-acore: 0.4941\n",
            "Epoch 00166\n",
            "Train: loss: 0.6931 | accuracy: 0.4944 | f-acore: 0.4941\n",
            "Epoch 00167\n",
            "Train: loss: 0.6931 | accuracy: 0.4944 | f-acore: 0.4941\n",
            "Epoch 00168\n",
            "Train: loss: 0.6931 | accuracy: 0.4944 | f-acore: 0.4941\n",
            "Epoch 00169\n",
            "Train: loss: 0.6931 | accuracy: 0.4937 | f-acore: 0.4934\n",
            "Epoch 00170\n",
            "Train: loss: 0.6931 | accuracy: 0.4937 | f-acore: 0.4934\n",
            "Epoch 00171\n",
            "Train: loss: 0.6931 | accuracy: 0.4937 | f-acore: 0.4934\n",
            "Epoch 00172\n",
            "Train: loss: 0.6931 | accuracy: 0.4937 | f-acore: 0.4934\n",
            "Epoch 00173\n",
            "Train: loss: 0.6931 | accuracy: 0.4937 | f-acore: 0.4934\n",
            "Epoch 00174\n",
            "Train: loss: 0.6931 | accuracy: 0.4937 | f-acore: 0.4934\n",
            "Epoch 00175\n",
            "Train: loss: 0.6931 | accuracy: 0.4937 | f-acore: 0.4934\n",
            "Epoch 00176\n",
            "Train: loss: 0.6931 | accuracy: 0.4937 | f-acore: 0.4934\n",
            "Epoch 00177\n",
            "Train: loss: 0.6931 | accuracy: 0.4931 | f-acore: 0.4928\n",
            "Epoch 00178\n",
            "Train: loss: 0.6931 | accuracy: 0.4931 | f-acore: 0.4928\n",
            "Epoch 00179\n",
            "Train: loss: 0.6931 | accuracy: 0.4924 | f-acore: 0.4921\n",
            "Epoch 00180\n",
            "Train: loss: 0.6931 | accuracy: 0.4924 | f-acore: 0.4921\n",
            "Epoch 00181\n",
            "Train: loss: 0.6931 | accuracy: 0.4924 | f-acore: 0.4921\n",
            "Epoch 00182\n",
            "Train: loss: 0.6931 | accuracy: 0.4924 | f-acore: 0.4921\n",
            "Epoch 00183\n",
            "Train: loss: 0.6931 | accuracy: 0.4924 | f-acore: 0.4921\n",
            "Epoch 00184\n",
            "Train: loss: 0.6931 | accuracy: 0.4924 | f-acore: 0.4921\n",
            "Epoch 00185\n",
            "Train: loss: 0.6931 | accuracy: 0.4924 | f-acore: 0.4921\n",
            "Epoch 00186\n",
            "Train: loss: 0.6931 | accuracy: 0.4917 | f-acore: 0.4914\n",
            "Epoch 00187\n",
            "Train: loss: 0.6931 | accuracy: 0.4917 | f-acore: 0.4914\n",
            "Epoch 00188\n",
            "Train: loss: 0.6931 | accuracy: 0.4917 | f-acore: 0.4914\n",
            "Epoch 00189\n",
            "Train: loss: 0.6931 | accuracy: 0.4917 | f-acore: 0.4914\n",
            "Epoch 00190\n",
            "Train: loss: 0.6931 | accuracy: 0.4917 | f-acore: 0.4914\n",
            "Epoch 00191\n",
            "Train: loss: 0.6931 | accuracy: 0.4917 | f-acore: 0.4914\n",
            "Epoch 00192\n",
            "Train: loss: 0.6931 | accuracy: 0.4917 | f-acore: 0.4914\n",
            "Epoch 00193\n",
            "Train: loss: 0.6931 | accuracy: 0.4917 | f-acore: 0.4914\n",
            "Epoch 00194\n",
            "Train: loss: 0.6931 | accuracy: 0.4917 | f-acore: 0.4914\n",
            "Epoch 00195\n",
            "Train: loss: 0.6931 | accuracy: 0.4917 | f-acore: 0.4914\n",
            "Epoch 00196\n",
            "Train: loss: 0.6931 | accuracy: 0.4911 | f-acore: 0.4908\n",
            "Epoch 00197\n",
            "Train: loss: 0.6931 | accuracy: 0.4911 | f-acore: 0.4908\n",
            "Epoch 00198\n",
            "Train: loss: 0.6931 | accuracy: 0.4911 | f-acore: 0.4908\n",
            "Epoch 00199\n",
            "Train: loss: 0.6931 | accuracy: 0.4911 | f-acore: 0.4908\n",
            "Epoch 00200\n",
            "Train: loss: 0.6931 | accuracy: 0.4911 | f-acore: 0.4908\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/drew/anaconda3/envs/CNNpredRep/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 00001\n",
            "Train: loss: 0.6933 | accuracy: 0.4944 | f-acore: 0.4828\n",
            "Epoch 00002\n",
            "Train: loss: 0.6933 | accuracy: 0.4957 | f-acore: 0.4815\n",
            "Epoch 00003\n",
            "Train: loss: 0.6933 | accuracy: 0.4983 | f-acore: 0.4810\n",
            "Epoch 00004\n",
            "Train: loss: 0.6933 | accuracy: 0.4983 | f-acore: 0.4791\n",
            "Epoch 00005\n",
            "Train: loss: 0.6933 | accuracy: 0.5023 | f-acore: 0.4801\n",
            "Epoch 00006\n",
            "Train: loss: 0.6933 | accuracy: 0.5063 | f-acore: 0.4815\n",
            "Epoch 00007\n",
            "Train: loss: 0.6933 | accuracy: 0.5043 | f-acore: 0.4783\n",
            "Epoch 00008\n",
            "Train: loss: 0.6932 | accuracy: 0.5043 | f-acore: 0.4773\n",
            "Epoch 00009\n",
            "Train: loss: 0.6932 | accuracy: 0.5056 | f-acore: 0.4774\n",
            "Epoch 00010\n",
            "Train: loss: 0.6932 | accuracy: 0.5056 | f-acore: 0.4760\n",
            "Epoch 00011\n",
            "Train: loss: 0.6932 | accuracy: 0.5089 | f-acore: 0.4769\n",
            "Epoch 00012\n",
            "Train: loss: 0.6932 | accuracy: 0.5069 | f-acore: 0.4716\n",
            "Epoch 00013\n",
            "Train: loss: 0.6932 | accuracy: 0.5050 | f-acore: 0.4673\n",
            "Epoch 00014\n",
            "Train: loss: 0.6932 | accuracy: 0.5056 | f-acore: 0.4653\n",
            "Epoch 00015\n",
            "Train: loss: 0.6932 | accuracy: 0.5036 | f-acore: 0.4599\n",
            "Epoch 00016\n",
            "Train: loss: 0.6932 | accuracy: 0.5043 | f-acore: 0.4559\n",
            "Epoch 00017\n",
            "Train: loss: 0.6932 | accuracy: 0.5056 | f-acore: 0.4539\n",
            "Epoch 00018\n",
            "Train: loss: 0.6932 | accuracy: 0.5050 | f-acore: 0.4495\n",
            "Epoch 00019\n",
            "Train: loss: 0.6932 | accuracy: 0.5036 | f-acore: 0.4443\n",
            "Epoch 00020\n",
            "Train: loss: 0.6932 | accuracy: 0.4990 | f-acore: 0.4344\n",
            "Epoch 00021\n",
            "Train: loss: 0.6932 | accuracy: 0.4931 | f-acore: 0.4233\n",
            "Epoch 00022\n",
            "Train: loss: 0.6932 | accuracy: 0.4983 | f-acore: 0.4244\n",
            "Epoch 00023\n",
            "Train: loss: 0.6932 | accuracy: 0.4937 | f-acore: 0.4116\n",
            "Epoch 00024\n",
            "Train: loss: 0.6932 | accuracy: 0.4970 | f-acore: 0.4110\n",
            "Epoch 00025\n",
            "Train: loss: 0.6932 | accuracy: 0.4957 | f-acore: 0.4015\n",
            "Epoch 00026\n",
            "Train: loss: 0.6932 | accuracy: 0.4937 | f-acore: 0.3927\n",
            "Epoch 00027\n",
            "Train: loss: 0.6932 | accuracy: 0.4917 | f-acore: 0.3801\n",
            "Epoch 00028\n",
            "Train: loss: 0.6932 | accuracy: 0.4898 | f-acore: 0.3675\n",
            "Epoch 00029\n",
            "Train: loss: 0.6932 | accuracy: 0.4865 | f-acore: 0.3564\n",
            "Epoch 00030\n",
            "Train: loss: 0.6932 | accuracy: 0.4924 | f-acore: 0.3556\n",
            "Epoch 00031\n",
            "Train: loss: 0.6932 | accuracy: 0.4924 | f-acore: 0.3496\n",
            "Epoch 00032\n",
            "Train: loss: 0.6932 | accuracy: 0.4931 | f-acore: 0.3457\n",
            "Epoch 00033\n",
            "Train: loss: 0.6932 | accuracy: 0.4931 | f-acore: 0.3414\n",
            "Epoch 00034\n",
            "Train: loss: 0.6932 | accuracy: 0.4924 | f-acore: 0.3378\n",
            "Epoch 00035\n",
            "Train: loss: 0.6932 | accuracy: 0.4937 | f-acore: 0.3362\n",
            "Epoch 00036\n",
            "Train: loss: 0.6932 | accuracy: 0.4931 | f-acore: 0.3348\n",
            "Epoch 00037\n",
            "Train: loss: 0.6932 | accuracy: 0.4924 | f-acore: 0.3311\n",
            "Epoch 00038\n",
            "Train: loss: 0.6932 | accuracy: 0.4924 | f-acore: 0.3311\n",
            "Epoch 00039\n",
            "Train: loss: 0.6932 | accuracy: 0.4931 | f-acore: 0.3314\n",
            "Epoch 00040\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3320\n",
            "Epoch 00041\n",
            "Train: loss: 0.6932 | accuracy: 0.4937 | f-acore: 0.3305\n",
            "Epoch 00042\n",
            "Train: loss: 0.6932 | accuracy: 0.4937 | f-acore: 0.3305\n",
            "Epoch 00043\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00044\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00045\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00046\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00047\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00048\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00049\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00050\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00051\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00052\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00053\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00054\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00055\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00056\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00057\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00058\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00059\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00060\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00061\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00062\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00063\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00064\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00065\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00066\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00067\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00068\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00069\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00070\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00071\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00072\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00073\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00074\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00075\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00076\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00077\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00078\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00079\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00080\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00081\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00082\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00083\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00084\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00085\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00086\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00087\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00088\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00089\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00090\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00091\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00092\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00093\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00094\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00095\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00096\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00097\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00098\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00099\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00100\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00101\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00102\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00103\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00104\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00105\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00106\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00107\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00108\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00109\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00110\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00111\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00112\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00113\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00114\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00115\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00116\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00117\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00118\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00119\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00120\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00121\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00122\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00123\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00124\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00125\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00126\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00127\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00128\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00129\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00130\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00131\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00132\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00133\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00134\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00135\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00136\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00137\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00138\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00139\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00140\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00141\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00142\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00143\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00144\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00145\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00146\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00147\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00148\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00149\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00150\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00151\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00152\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00153\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00154\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00155\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00156\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00157\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00158\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00159\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00160\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00161\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00162\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00163\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00164\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00165\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00166\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00167\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00168\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00169\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00170\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00171\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00172\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00173\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00174\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00175\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00176\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00177\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00178\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00179\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00180\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00181\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00182\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00183\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00184\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00185\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00186\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00187\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00188\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00189\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00190\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00191\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00192\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00193\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00194\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00195\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00196\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00197\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00198\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00199\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n",
            "Epoch 00200\n",
            "Train: loss: 0.6932 | accuracy: 0.4944 | f-acore: 0.3308\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/drew/anaconda3/envs/CNNpredRep/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 00001\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.5027\n",
            "Epoch 00002\n",
            "Train: loss: 0.6930 | accuracy: 0.5155 | f-acore: 0.4961\n",
            "Epoch 00003\n",
            "Train: loss: 0.6930 | accuracy: 0.5129 | f-acore: 0.4888\n",
            "Epoch 00004\n",
            "Train: loss: 0.6930 | accuracy: 0.5142 | f-acore: 0.4843\n",
            "Epoch 00005\n",
            "Train: loss: 0.6929 | accuracy: 0.5168 | f-acore: 0.4831\n",
            "Epoch 00006\n",
            "Train: loss: 0.6929 | accuracy: 0.5188 | f-acore: 0.4804\n",
            "Epoch 00007\n",
            "Train: loss: 0.6929 | accuracy: 0.5195 | f-acore: 0.4763\n",
            "Epoch 00008\n",
            "Train: loss: 0.6929 | accuracy: 0.5188 | f-acore: 0.4709\n",
            "Epoch 00009\n",
            "Train: loss: 0.6929 | accuracy: 0.5162 | f-acore: 0.4612\n",
            "Epoch 00010\n",
            "Train: loss: 0.6929 | accuracy: 0.5122 | f-acore: 0.4515\n",
            "Epoch 00011\n",
            "Train: loss: 0.6929 | accuracy: 0.5089 | f-acore: 0.4419\n",
            "Epoch 00012\n",
            "Train: loss: 0.6929 | accuracy: 0.5083 | f-acore: 0.4367\n",
            "Epoch 00013\n",
            "Train: loss: 0.6929 | accuracy: 0.5076 | f-acore: 0.4306\n",
            "Epoch 00014\n",
            "Train: loss: 0.6929 | accuracy: 0.5135 | f-acore: 0.4327\n",
            "Epoch 00015\n",
            "Train: loss: 0.6929 | accuracy: 0.5155 | f-acore: 0.4313\n",
            "Epoch 00016\n",
            "Train: loss: 0.6929 | accuracy: 0.5162 | f-acore: 0.4276\n",
            "Epoch 00017\n",
            "Train: loss: 0.6929 | accuracy: 0.5162 | f-acore: 0.4240\n",
            "Epoch 00018\n",
            "Train: loss: 0.6929 | accuracy: 0.5195 | f-acore: 0.4239\n",
            "Epoch 00019\n",
            "Train: loss: 0.6929 | accuracy: 0.5201 | f-acore: 0.4213\n",
            "Epoch 00020\n",
            "Train: loss: 0.6929 | accuracy: 0.5195 | f-acore: 0.4178\n",
            "Epoch 00021\n",
            "Train: loss: 0.6929 | accuracy: 0.5195 | f-acore: 0.4131\n",
            "Epoch 00022\n",
            "Train: loss: 0.6929 | accuracy: 0.5188 | f-acore: 0.4103\n",
            "Epoch 00023\n",
            "Train: loss: 0.6929 | accuracy: 0.5228 | f-acore: 0.4101\n",
            "Epoch 00024\n",
            "Train: loss: 0.6929 | accuracy: 0.5215 | f-acore: 0.4051\n",
            "Epoch 00025\n",
            "Train: loss: 0.6930 | accuracy: 0.5221 | f-acore: 0.4046\n",
            "Epoch 00026\n",
            "Train: loss: 0.6930 | accuracy: 0.5188 | f-acore: 0.3974\n",
            "Epoch 00027\n",
            "Train: loss: 0.6930 | accuracy: 0.5195 | f-acore: 0.3960\n",
            "Epoch 00028\n",
            "Train: loss: 0.6930 | accuracy: 0.5208 | f-acore: 0.3940\n",
            "Epoch 00029\n",
            "Train: loss: 0.6930 | accuracy: 0.5201 | f-acore: 0.3927\n",
            "Epoch 00030\n",
            "Train: loss: 0.6930 | accuracy: 0.5188 | f-acore: 0.3891\n",
            "Epoch 00031\n",
            "Train: loss: 0.6930 | accuracy: 0.5182 | f-acore: 0.3888\n",
            "Epoch 00032\n",
            "Train: loss: 0.6930 | accuracy: 0.5188 | f-acore: 0.3891\n",
            "Epoch 00033\n",
            "Train: loss: 0.6930 | accuracy: 0.5201 | f-acore: 0.3889\n",
            "Epoch 00034\n",
            "Train: loss: 0.6930 | accuracy: 0.5195 | f-acore: 0.3876\n",
            "Epoch 00035\n",
            "Train: loss: 0.6930 | accuracy: 0.5195 | f-acore: 0.3867\n",
            "Epoch 00036\n",
            "Train: loss: 0.6930 | accuracy: 0.5195 | f-acore: 0.3867\n",
            "Epoch 00037\n",
            "Train: loss: 0.6930 | accuracy: 0.5188 | f-acore: 0.3844\n",
            "Epoch 00038\n",
            "Train: loss: 0.6930 | accuracy: 0.5182 | f-acore: 0.3830\n",
            "Epoch 00039\n",
            "Train: loss: 0.6930 | accuracy: 0.5188 | f-acore: 0.3834\n",
            "Epoch 00040\n",
            "Train: loss: 0.6930 | accuracy: 0.5182 | f-acore: 0.3811\n",
            "Epoch 00041\n",
            "Train: loss: 0.6930 | accuracy: 0.5182 | f-acore: 0.3811\n",
            "Epoch 00042\n",
            "Train: loss: 0.6930 | accuracy: 0.5201 | f-acore: 0.3831\n",
            "Epoch 00043\n",
            "Train: loss: 0.6930 | accuracy: 0.5195 | f-acore: 0.3818\n",
            "Epoch 00044\n",
            "Train: loss: 0.6930 | accuracy: 0.5188 | f-acore: 0.3804\n",
            "Epoch 00045\n",
            "Train: loss: 0.6930 | accuracy: 0.5201 | f-acore: 0.3811\n",
            "Epoch 00046\n",
            "Train: loss: 0.6931 | accuracy: 0.5195 | f-acore: 0.3798\n",
            "Epoch 00047\n",
            "Train: loss: 0.6931 | accuracy: 0.5215 | f-acore: 0.3818\n",
            "Epoch 00048\n",
            "Train: loss: 0.6931 | accuracy: 0.5215 | f-acore: 0.3818\n",
            "Epoch 00049\n",
            "Train: loss: 0.6931 | accuracy: 0.5215 | f-acore: 0.3818\n",
            "Epoch 00050\n",
            "Train: loss: 0.6931 | accuracy: 0.5208 | f-acore: 0.3805\n",
            "Epoch 00051\n",
            "Train: loss: 0.6931 | accuracy: 0.5208 | f-acore: 0.3805\n",
            "Epoch 00052\n",
            "Train: loss: 0.6931 | accuracy: 0.5208 | f-acore: 0.3805\n",
            "Epoch 00053\n",
            "Train: loss: 0.6931 | accuracy: 0.5208 | f-acore: 0.3805\n",
            "Epoch 00054\n",
            "Train: loss: 0.6931 | accuracy: 0.5208 | f-acore: 0.3805\n",
            "Epoch 00055\n",
            "Train: loss: 0.6931 | accuracy: 0.5195 | f-acore: 0.3788\n",
            "Epoch 00056\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3784\n",
            "Epoch 00057\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3784\n",
            "Epoch 00058\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3784\n",
            "Epoch 00059\n",
            "Train: loss: 0.6931 | accuracy: 0.5182 | f-acore: 0.3781\n",
            "Epoch 00060\n",
            "Train: loss: 0.6931 | accuracy: 0.5182 | f-acore: 0.3781\n",
            "Epoch 00061\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3760\n",
            "Epoch 00062\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3760\n",
            "Epoch 00063\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3760\n",
            "Epoch 00064\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3760\n",
            "Epoch 00065\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3760\n",
            "Epoch 00066\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3760\n",
            "Epoch 00067\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3774\n",
            "Epoch 00068\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3760\n",
            "Epoch 00069\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3774\n",
            "Epoch 00070\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3774\n",
            "Epoch 00071\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3774\n",
            "Epoch 00072\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3760\n",
            "Epoch 00073\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3760\n",
            "Epoch 00074\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3764\n",
            "Epoch 00075\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3764\n",
            "Epoch 00076\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3764\n",
            "Epoch 00077\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3750\n",
            "Epoch 00078\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3750\n",
            "Epoch 00079\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3750\n",
            "Epoch 00080\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3750\n",
            "Epoch 00081\n",
            "Train: loss: 0.6931 | accuracy: 0.5155 | f-acore: 0.3747\n",
            "Epoch 00082\n",
            "Train: loss: 0.6931 | accuracy: 0.5155 | f-acore: 0.3747\n",
            "Epoch 00083\n",
            "Train: loss: 0.6931 | accuracy: 0.5155 | f-acore: 0.3747\n",
            "Epoch 00084\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3760\n",
            "Epoch 00085\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3760\n",
            "Epoch 00086\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3760\n",
            "Epoch 00087\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3774\n",
            "Epoch 00088\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3777\n",
            "Epoch 00089\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3777\n",
            "Epoch 00090\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3777\n",
            "Epoch 00091\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3777\n",
            "Epoch 00092\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3777\n",
            "Epoch 00093\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3777\n",
            "Epoch 00094\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3777\n",
            "Epoch 00095\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3777\n",
            "Epoch 00096\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3777\n",
            "Epoch 00097\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3777\n",
            "Epoch 00098\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3777\n",
            "Epoch 00099\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3777\n",
            "Epoch 00100\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3777\n",
            "Epoch 00101\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3777\n",
            "Epoch 00102\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3777\n",
            "Epoch 00103\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3777\n",
            "Epoch 00104\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3777\n",
            "Epoch 00105\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3804\n",
            "Epoch 00106\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3804\n",
            "Epoch 00107\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3804\n",
            "Epoch 00108\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3804\n",
            "Epoch 00109\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3804\n",
            "Epoch 00110\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3804\n",
            "Epoch 00111\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3804\n",
            "Epoch 00112\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3804\n",
            "Epoch 00113\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3804\n",
            "Epoch 00114\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3804\n",
            "Epoch 00115\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3804\n",
            "Epoch 00116\n",
            "Train: loss: 0.6931 | accuracy: 0.5195 | f-acore: 0.3818\n",
            "Epoch 00117\n",
            "Train: loss: 0.6931 | accuracy: 0.5195 | f-acore: 0.3818\n",
            "Epoch 00118\n",
            "Train: loss: 0.6931 | accuracy: 0.5195 | f-acore: 0.3818\n",
            "Epoch 00119\n",
            "Train: loss: 0.6931 | accuracy: 0.5195 | f-acore: 0.3818\n",
            "Epoch 00120\n",
            "Train: loss: 0.6931 | accuracy: 0.5201 | f-acore: 0.3831\n",
            "Epoch 00121\n",
            "Train: loss: 0.6931 | accuracy: 0.5201 | f-acore: 0.3831\n",
            "Epoch 00122\n",
            "Train: loss: 0.6931 | accuracy: 0.5195 | f-acore: 0.3828\n",
            "Epoch 00123\n",
            "Train: loss: 0.6931 | accuracy: 0.5195 | f-acore: 0.3828\n",
            "Epoch 00124\n",
            "Train: loss: 0.6931 | accuracy: 0.5195 | f-acore: 0.3828\n",
            "Epoch 00125\n",
            "Train: loss: 0.6931 | accuracy: 0.5195 | f-acore: 0.3828\n",
            "Epoch 00126\n",
            "Train: loss: 0.6931 | accuracy: 0.5201 | f-acore: 0.3841\n",
            "Epoch 00127\n",
            "Train: loss: 0.6931 | accuracy: 0.5201 | f-acore: 0.3841\n",
            "Epoch 00128\n",
            "Train: loss: 0.6931 | accuracy: 0.5201 | f-acore: 0.3841\n",
            "Epoch 00129\n",
            "Train: loss: 0.6931 | accuracy: 0.5201 | f-acore: 0.3841\n",
            "Epoch 00130\n",
            "Train: loss: 0.6931 | accuracy: 0.5201 | f-acore: 0.3841\n",
            "Epoch 00131\n",
            "Train: loss: 0.6931 | accuracy: 0.5195 | f-acore: 0.3837\n",
            "Epoch 00132\n",
            "Train: loss: 0.6931 | accuracy: 0.5201 | f-acore: 0.3851\n",
            "Epoch 00133\n",
            "Train: loss: 0.6931 | accuracy: 0.5201 | f-acore: 0.3851\n",
            "Epoch 00134\n",
            "Train: loss: 0.6931 | accuracy: 0.5195 | f-acore: 0.3847\n",
            "Epoch 00135\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3844\n",
            "Epoch 00136\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3844\n",
            "Epoch 00137\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3844\n",
            "Epoch 00138\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3844\n",
            "Epoch 00139\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3844\n",
            "Epoch 00140\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3844\n",
            "Epoch 00141\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3844\n",
            "Epoch 00142\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3844\n",
            "Epoch 00143\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3844\n",
            "Epoch 00144\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3844\n",
            "Epoch 00145\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3844\n",
            "Epoch 00146\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3844\n",
            "Epoch 00147\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3844\n",
            "Epoch 00148\n",
            "Train: loss: 0.6931 | accuracy: 0.5188 | f-acore: 0.3844\n",
            "Epoch 00149\n",
            "Train: loss: 0.6931 | accuracy: 0.5182 | f-acore: 0.3840\n",
            "Epoch 00150\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3836\n",
            "Epoch 00151\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3836\n",
            "Epoch 00152\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3836\n",
            "Epoch 00153\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3836\n",
            "Epoch 00154\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3836\n",
            "Epoch 00155\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3836\n",
            "Epoch 00156\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3836\n",
            "Epoch 00157\n",
            "Train: loss: 0.6931 | accuracy: 0.5175 | f-acore: 0.3836\n",
            "Epoch 00158\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3833\n",
            "Epoch 00159\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3833\n",
            "Epoch 00160\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3833\n",
            "Epoch 00161\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3833\n",
            "Epoch 00162\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3833\n",
            "Epoch 00163\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3833\n",
            "Epoch 00164\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3833\n",
            "Epoch 00165\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3833\n",
            "Epoch 00166\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3833\n",
            "Epoch 00167\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3833\n",
            "Epoch 00168\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3833\n",
            "Epoch 00169\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3833\n",
            "Epoch 00170\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3833\n",
            "Epoch 00171\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3833\n",
            "Epoch 00172\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3833\n",
            "Epoch 00173\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3833\n",
            "Epoch 00174\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3833\n",
            "Epoch 00175\n",
            "Train: loss: 0.6931 | accuracy: 0.5168 | f-acore: 0.3833\n",
            "Epoch 00176\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00177\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00178\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00179\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00180\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00181\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00182\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00183\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00184\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00185\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00186\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00187\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00188\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00189\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00190\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00191\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00192\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00193\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00194\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00195\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00196\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00197\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00198\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00199\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n",
            "Epoch 00200\n",
            "Train: loss: 0.6931 | accuracy: 0.5162 | f-acore: 0.3829\n"
          ]
        }
      ],
      "source": [
        "#Main.py script\n",
        "#Training loop\n",
        "import models #TODO: Put this at top of imports\n",
        "\n",
        "\n",
        "gen_num = 1 #TODO: Define this in the Config file\n",
        "for iteration in range(gen_num):\n",
        "\n",
        "\n",
        "  for index, _ in y_train.items():\n",
        "    #Get Model output path\n",
        "    model_path = next_file(config['model']['type'] + index, join(\"./Results/\", os.path.basename(config_path)))\n",
        "    # for b, a, in train_dataloader[index]:\n",
        "    #   print(\"opppppppppppppppppppppppppppppppppppppppppppppppppppppppp\")\n",
        "    train(config, train_data[index])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
